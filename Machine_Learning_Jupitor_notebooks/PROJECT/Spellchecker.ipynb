{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXTBLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\rajar\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\rajar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\rajar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\rajar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajar\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.47.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/textblob/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/textblob/\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: hell0\n",
      "corrected text: hell\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob \n",
    "\n",
    "a = \"hell0\"\t\t # incorrect spelling \n",
    "print(\"original text: \"+str(a)) \n",
    "\n",
    "b = TextBlob(a) \n",
    "\n",
    "# prints the corrected spelling \n",
    "print(\"corrected text: \"+str(b.correct())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PY_SPELL_CHECKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.5.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_SpellChecker__edit_distance_alt',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_case_sensitive',\n",
       " '_check_if_should_check',\n",
       " '_distance',\n",
       " '_tokenizer',\n",
       " '_word_frequency',\n",
       " 'candidates',\n",
       " 'correction',\n",
       " 'distance',\n",
       " 'edit_distance_1',\n",
       " 'edit_distance_2',\n",
       " 'export',\n",
       " 'known',\n",
       " 'split_words',\n",
       " 'unknown',\n",
       " 'word_frequency',\n",
       " 'word_probability']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(SpellChecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n",
      "{'watt', 'wat', 'wath', 'warr', 'wart', 'water', 'war'}\n",
      "write\n",
      "{'write', 'wrote', 'wroe', 'wre', 'arte', 'rte'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spell = SpellChecker() \n",
    "\n",
    "# find those words that may be misspelled \n",
    "misspelled = spell.unknown([\"prt\", \"watr\", \"study\", \"wrte\"]) \n",
    "\n",
    "for word in misspelled: \n",
    "    # Get the one `most likely` answer \n",
    "    print(spell.correction(word)) \n",
    "\n",
    "    # Get a list of `likely` options \n",
    "    print(spell.candidates(word)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pure python spell checking based on per norris s blow post on setting up a simple spell checking algorithm it uses a levenshtein distance algorithm to find permutations within an edit distance of 2 from the original word it then compares all permutations insertions deletions replacements and transpositions to known words in a word frequency list those words that are found more often in the frequency list are more likely the correct results spellchecker supports multiple languages including english spanish german french and portuguese dictionaries were generated using the wordfrequency project on citub']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "spell = SpellChecker()\n",
    "\n",
    "def reTokenize(doc):\n",
    "    tokens = WORD.findall(doc)\n",
    "    return tokens\n",
    "\n",
    "text = [\"Pure Python Spell Chcking based on Pter Norvigâ€™s blog post on setting up a simple spell checking algorithm.\"+\n",
    "\n",
    "\"It uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results.\"+\n",
    "\n",
    "\"pyspellchecker supports multiple languages including English, Spanish, German, French, and Portuguese. Dictionaries were generated using the WordFrequency project on GitHub.\"]\n",
    "\n",
    "def spell_correct(text):\n",
    "    sptext =  [' '.join([spell.correction(w).lower() for w in reTokenize(doc)])  for doc in text]    \n",
    "    return sptext    \n",
    "\n",
    "print(spell_correct(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi welcome to spelling', 'this is just an example but consider a veri big corpus']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "spell=Speller(lang=\"en\")\n",
    "WORD = re.compile(r'\\w+')\n",
    "def reTokenize(doc):\n",
    "    tokens = WORD.findall(doc)\n",
    "    return tokens\n",
    "\n",
    "text = [\"Hi, welcmoe to speling.\",\"This is jsut an exapmle, but cosnider a veri big coprus.\"]\n",
    "def spell_correct(text):\n",
    "    sptext = []\n",
    "    for doc in text:\n",
    "        sptext.append(' '.join([spell(w).lower() for w in reTokenize(doc)]))      \n",
    "    return sptext    \n",
    "\n",
    "print(spell_correct(text)) \n",
    "\n",
    "#Output\n",
    "#['hi welcome to spelling', 'this is just an example but consider a veri big corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install autocorrect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
